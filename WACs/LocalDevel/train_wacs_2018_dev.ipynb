{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import codecs\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import ConfigParser\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigParser.SafeConfigParser()\n",
    "my_config = '/Users/das/work/svn/Gits/a_Projects/Projects/Infrastructure/dsg-vision/Config/default.cfg'\n",
    "with codecs.open(my_config, 'r', encoding='utf-8') as f:\n",
    "    config.readfp(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "preproc_path = dsgv_home + '/Preproc/PreprocOut/'\n",
    "feats_path = dsgv_home + '/ExtractFeats/ExtractOut/'\n",
    "\n",
    "# The first features in the image feature Xs encode the region ID\n",
    "ID_FEATS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(preproc_path + 'refcoco_splits.json', 'r') as f:\n",
    "    rc_splits = json.load(f)\n",
    "    \n",
    "# X = np.load(feats_path + 'mscoco_vgg19-fc2.npz')['arr_0']\n",
    "X = np.load(feats_path + 'mscoco_bbdf_rsn50-flatten_1.npz')['arr_0']\n",
    "\n",
    "refcoco_refdf = pd.read_json(preproc_path + 'refcoco_refdf.json.gz',\n",
    "                         typ='frame', orient='split', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_X_by_filelist(X, filelist):\n",
    "    tmp_df = pd.DataFrame(X)\n",
    "    return np.array(tmp_df[tmp_df.iloc[:,1].isin(filelist)])\n",
    "\n",
    "def filter_refdf_by_filelist(refdf, filelist):\n",
    "    return pd.merge(refdf, pd.DataFrame(filelist, columns=['image_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = filter_X_by_filelist(X, rc_splits['train'])\n",
    "refdf_train = filter_refdf_by_filelist(refcoco_refdf, rc_splits['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2den(refdf, refcol='refexp', regcol='region_id'):\n",
    "    '''Given refdf, returns dict of occurences (id triples) of words from expressions.'''\n",
    "    word2den = defaultdict(list)\n",
    "    for _, row in refdf.iterrows():\n",
    "        exprlist = row[refcol].split()\n",
    "        # TODO: Could take filter function that filters out some occurences.\n",
    "        #   E.g., tagger that tags whole expression & returns only the nouns.\n",
    "        for word in exprlist:\n",
    "            word_den_list = word2den[word].append((row['i_corpus'],\n",
    "                                                   row['image_id'],\n",
    "                                                   row[regcol]))\n",
    "    return {k: list(set(v)) for k,v in word2den.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 1.32 s, total: 20.9 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2den = create_word2den(refdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X_id_index(X, id_feats=ID_FEATS):\n",
    "    return dict(zip([tuple(e) for e in X[:,:id_feats].astype(int).tolist()], range(len(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 263 ms, sys: 11.3 ms, total: 275 ms\n",
      "Wall time: 273 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_idx = make_X_id_index(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_matrix(X, X_idx, word2den, wordlist):\n",
    "    mask_matrix = []\n",
    "    for this_word in wordlist:\n",
    "        this_word_vec = np.zeros(len(X))\n",
    "        if this_word in word2den:\n",
    "            this_word_vec[[X_idx[i] for i in word2den[this_word] if i in X_idx]] = 1\n",
    "        mask_matrix.append(this_word_vec)\n",
    "    mask_matrix = np.array(mask_matrix, dtype=bool)\n",
    "    return mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.29 s, sys: 6.34 s, total: 9.63 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mask_matrix = make_mask_matrix(X_t, X_idx, word2den, word2den.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## N.B.: Replace with make_X_for_word from below! Can be used for extracting\n",
    "##   test data as well..\n",
    "\n",
    "def make_train_for_word(X, word2den, mask_matrix, word, neg_max=20000):\n",
    "    if word not in word2den:\n",
    "        #raise ValueError(\"No mask available for this word! (%s)\" % (word))\n",
    "        print \"Error!! No mask available for this word! (%s)\" % (word)\n",
    "        return None\n",
    "    this_mask = mask_matrix[word2den.keys().index(word)]\n",
    "    X_pos = X[this_mask, ID_FEATS:]\n",
    "    y_pos = np.ones(len(X_pos), dtype=int)\n",
    "    \n",
    "    neg_indx = np.arange(mask_matrix.shape[1])[~this_mask]\n",
    "    np.random.shuffle(neg_indx)\n",
    "    X_neg = X[neg_indx[:neg_max], ID_FEATS:]\n",
    "    y_neg = np.zeros(len(X_neg), dtype=int)\n",
    "\n",
    "    X_out = np.concatenate([X_pos, X_neg], axis=0)\n",
    "    y_out = np.concatenate([y_pos, y_neg])\n",
    "    return shuffle(X_out, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -T prof1 -f make_train_for_word X_this_w, y_this_w = make_train_for_word(X_t, word2den, mask_matrix, 'cow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sped up `make_train_for_word` by limiting the size of the negative set. Was 40secs, now 3 secs. Still slower than I would like. But selecting a very large portion of the matrix with a boolean vector seems to be very slow. Maybe there is a more clever way to do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 378 ms, sys: 871 ms, total: 1.25 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_this_w, y_this_w = make_train_for_word(X_t, word2den, mask_matrix, 'cow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9364, 166354)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the set of words for which WAC is trained, by frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 40\n",
    "\n",
    "counts = mask_matrix.sum(axis=1)\n",
    "\n",
    "wordlist = np.array(word2den.keys())[counts > min_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = linear_model.LogisticRegression(penalty='l1', warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 20896\n",
      "wooden 20053\n",
      "hanging 20059\n",
      "second 21922\n",
      "kids 20056\n",
      "glass 20533\n",
      "hot 20186\n",
      "wine 20184\n",
      "backpack 20100\n",
      "silver 20123\n",
      "CPU times: user 2min 11s, sys: 15.2 s, total: 2min 26s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wacs = []\n",
    "for this_word in wordlist[:10]:\n",
    "    # print \".\",\n",
    "    X_this_w, y_this_w = make_train_for_word(X_t, word2den, mask_matrix, this_word)\n",
    "    print this_word, X_this_w.shape[0]\n",
    "    classifier = linear_model.LogisticRegression(penalty='l1', warm_start=True)\n",
    "    this_wac = classifier.fit(X_this_w, y_this_w)\n",
    "    wacs.append((this_word, this_wac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_this_word(X, word2den, mask_matrix, this_word):\n",
    "    X_this_w, y_this_w = make_train_for_word(X_t, word2den, mask_matrix, this_word)\n",
    "    print this_word, X_this_w.shape[0]\n",
    "    classifier = linear_model.LogisticRegression(penalty='l1', warm_start=True)\n",
    "    this_wac = classifier.fit(X_this_w, y_this_w)\n",
    "    return (this_word, y_this_w.sum(), len(X_this_w), this_wac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 20896\n",
      "wooden 20053\n",
      "hanging 20059\n",
      "second 21922\n",
      "kids 20056\n",
      "glass 20533\n",
      "hot 20186\n",
      "wine 20184\n",
      "backpack 20100\n",
      "silver 20123\n",
      "CPU times: user 2min 41s, sys: 9.65 s, total: 2min 51s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wacs = [train_this_word(X, word2den, mask_matrix, this_word)\\\n",
    "        for this_word in wordlist[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wooden 20053\n",
      "hanging 20059\n",
      "yellowsecond 21922\n",
      " 20896\n",
      "kids 20056\n",
      "glass 20533\n",
      "hot 20186\n",
      "wine 20184\n",
      "backpack 20100\n",
      "silver 20123\n",
      "CPU times: user 3min 44s, sys: 19.3 s, total: 4min 4s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wacs = Parallel(n_jobs=4, require='sharedmem', prefer='threads')\\\n",
    "               (delayed(train_this_word)(X, word2den, mask_matrix, this_word)\\\n",
    "                for this_word in wordlist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wooden 20053\n",
      "yellow 20896\n",
      "hanging 20059\n",
      "second 21922\n",
      "kids 20056\n",
      "glass 20533\n",
      "hot 20186\n",
      "wine 20184\n",
      "backpack 20100\n",
      "silver 20123\n",
      "CPU times: user 2min 57s, sys: 12.4 s, total: 3min 10s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wacs = Parallel(n_jobs=2, require='sharedmem', prefer='threads')\\\n",
    "               (delayed(train_this_word)(X, word2den, mask_matrix, this_word)\\\n",
    "                for this_word in wordlist[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 20896\n",
      "wooden 20053\n",
      "hanging 20059\n",
      "second 21922\n",
      "kids 20056\n",
      "glass 20533\n",
      "hot 20186\n",
      "wine 20184\n",
      "backpack 20100\n",
      "silver 20123\n",
      "CPU times: user 2min 9s, sys: 11.8 s, total: 2min 21s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wacs = Parallel(n_jobs=1, require='sharedmem', prefer='threads')\\\n",
    "               (delayed(train_this_word)(X, word2den, mask_matrix, this_word)\\\n",
    "                for this_word in wordlist[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributing over two cores seems to be worth it. Diminishing returns for more cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could still try to train on keras? https://gist.github.com/fchollet/b7507f373a3446097f26840330c1c378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'yellow',\n",
       " 896,\n",
       " 20896,\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wacs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining tasks:\n",
    "\n",
    "* evaluation? Run models on training data (with smaller n_neg... maybe balanced? should be option in make_train... which might better be called make_word_dataset...)\n",
    "* how to persist models.. Write out weight matrix and wordlist to disk, as numpy structures? scikit learn objects not very well serialisable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_for_word(X, word2den, mask_matrix, word, neg_max=20000):\n",
    "    if word not in word2den:\n",
    "        #raise ValueError(\"No mask available for this word! (%s)\" % (word))\n",
    "        print \"Error!! No mask available for this word! (%s)\" % (word)\n",
    "        return None\n",
    "    this_mask = mask_matrix[word2den.keys().index(word)]\n",
    "    X_pos = X[this_mask, ID_FEATS:]\n",
    "    y_pos = np.ones(len(X_pos), dtype=int)\n",
    "    \n",
    "    if neg_max == 0:\n",
    "        return X_pos, y_pos\n",
    "    \n",
    "    if neg_max == 'balanced':\n",
    "        neg_max = len(y_pos)\n",
    "\n",
    "    neg_indx = np.arange(mask_matrix.shape[1])[~this_mask]\n",
    "    np.random.shuffle(neg_indx)\n",
    "    X_neg = X[neg_indx[:neg_max], ID_FEATS:]\n",
    "    y_neg = np.zeros(len(X_neg), dtype=int)\n",
    "\n",
    "    X_out = np.concatenate([X_pos, X_neg], axis=0)\n",
    "    y_out = np.concatenate([y_pos, y_neg])\n",
    "    return shuffle(X_out, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 896 \t0.9899553571428571\n",
      "wooden 53 \t1.0\n",
      "hanging 59 \t1.0\n",
      "second 1922 \t0.9419875130072841\n",
      "kids 56 \t1.0\n",
      "glass 533 \t0.9971857410881801\n",
      "hot 186 \t1.0\n",
      "wine 184 \t0.9945652173913043\n",
      "backpack 100 \t0.995\n",
      "silver 123 \t1.0\n"
     ]
    }
   ],
   "source": [
    "for this_word, npos, _, this_clsf in wacs:\n",
    "    print this_word, npos, '\\t',\n",
    "    X_tst, y_tst = get_X_for_word(X_t, word2den, mask_matrix, this_word, neg_max='balanced')\n",
    "    print this_clsf.score(X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 896 \t0.9380580357142857\n",
      "wooden 53 \t1.0\n",
      "hanging 59 \t1.0\n",
      "second 1922 \t0.7557232049947971\n",
      "kids 56 \t0.9910714285714286\n",
      "glass 533 \t0.9878048780487805\n",
      "hot 186 \t0.9973118279569892\n",
      "wine 184 \t0.9972826086956522\n",
      "backpack 100 \t0.995\n",
      "silver 123 \t0.9796747967479674\n"
     ]
    }
   ],
   "source": [
    "for this_word, npos, _, this_clsf in wacs:\n",
    "    print this_word, npos, '\\t',\n",
    "    X_tst, y_tst = get_X_for_word(X_t, word2den, mask_matrix, this_word, neg_max='balanced')\n",
    "    print this_clsf.score(X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on training data (!) unsuprisingly pretty good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_all_test = rc_splits['testA'] + rc_splits['testB']\n",
    "X_ts = filter_X_by_filelist(X, rc_all_test)\n",
    "refdf_test = filter_refdf_by_filelist(refcoco_refdf, rc_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2den_ts = create_word2den(refdf_test)\n",
    "X_idx_ts = make_X_id_index(X_ts)\n",
    "mask_matrix_ts = make_mask_matrix(X_ts, X_idx_ts, word2den_ts, word2den_ts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 896 \t0.6483516483516484\n",
      "wooden 53 \t0.6\n",
      "hanging 59 \t0.5\n",
      "second 1922 \t0.6525\n",
      "kids 56 \t0.5\n",
      "glass 533 \t0.7073170731707317\n",
      "hot 186 \t0.65\n",
      "wine 184 \t0.71875\n",
      "backpack 100 \t0.5833333333333334\n",
      "silver 123 \t0.5\n"
     ]
    }
   ],
   "source": [
    "for this_word, npos, _, this_clsf in wacs:\n",
    "    print this_word, npos, '\\t',\n",
    "    X_tst, y_tst = get_X_for_word(X_ts, word2den_ts, mask_matrix_ts, this_word, neg_max='balanced')\n",
    "    print this_clsf.score(X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow 896 \t0.6813186813186813\n",
      "wooden 53 \t0.6\n",
      "hanging 59 \t0.5\n",
      "second 1922 \t0.5925\n",
      "kids 56 \t0.55\n",
      "glass 533 \t0.7195121951219512\n",
      "hot 186 \t0.675\n",
      "wine 184 \t0.75\n",
      "backpack 100 \t0.5833333333333334\n",
      "silver 123 \t0.5\n"
     ]
    }
   ],
   "source": [
    "for this_word, npos, _, this_clsf in wacs:\n",
    "    print this_word, npos, '\\t',\n",
    "    X_tst, y_tst = get_X_for_word(X_ts, word2den_ts, mask_matrix_ts, this_word, neg_max='balanced')\n",
    "    print this_clsf.score(X_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's looking not at all so great on the test set... (Although this is not the evaluation that is of ultimate interest here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- persisting the trained model... As weight matrix? (Together with wordlist & other interesting data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_wac = wacs[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.53120489])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_wac.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2056)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([np.append(this_wac.coef_, this_wac.intercept_) \\\n",
    "          for this_wac in [w[3] for w in wacs]]).shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((u'yellow', 896, 20896),)\n",
      "((u'wooden', 53, 20053),)\n",
      "((u'hanging', 59, 20059),)\n",
      "((u'second', 1922, 21922),)\n",
      "((u'kids', 56, 20056),)\n",
      "((u'glass', 533, 20533),)\n",
      "((u'hot', 186, 20186),)\n",
      "((u'wine', 184, 20184),)\n",
      "((u'backpack', 100, 20100),)\n",
      "((u'silver', 123, 20123),)\n"
     ]
    }
   ],
   "source": [
    "for this_wac in zip([e[:-1] for e in wacs]):\n",
    "    print this_wac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = [e[:-1] for e in wacs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = {\n",
    "        'rcorp': 'refcoco',        # ref corpus\n",
    "        'cnn': 'rsn50-flatten_1',  # CNN used for vision feats\n",
    "        'rel':   'excl',           # exclude relational expressions\n",
    "        'wrdl':  'min',            # wordlist: minimal n occurrences...\n",
    "        'wprm':  40,               # ... 40 times\n",
    "        'clsf':  'logreg-l1',      # logistic regression, l1 regularized\n",
    "        'nneg':  20000,            # maximally 20k neg instances\n",
    "        'nsrc':  'randmax',        # ... randomly selected\n",
    "        'notes': ''\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"nneg\": 20000, \"rcorp\": \"refcoco\", \"clsf\": \"logreg-l1\", \"rel\": \"excl\", \"cnn\": \"rsn50-flatten_1\", \"notes\": \"\", \"wrdl\": \"min\", \"nsrc\": \"randmax\", \"wprm\": 40}, [[\"yellow\", 896, 20896], [\"wooden\", 53, 20053], [\"hanging\", 59, 20059], [\"second\", 1922, 21922], [\"kids\", 56, 20056], [\"glass\", 533, 20533], [\"hot\", 186, 20186], [\"wine\", 184, 20184], [\"backpack\", 100, 20100], [\"silver\", 123, 20123]]]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps((model, wl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmatrix_ld = np.load('../ModelsOut/mod01_refcoco.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightmatrix = weightmatrix_ld['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2056)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightmatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196118, 2058)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
